# configs/dataset/default_dataset.yaml
# name: "ccdv/arxiv-classification" # No longer used for HDF5 direct loading
# config_name: "no_ref" # No longer used
# subset: "no_ref" # No longer used
# num_top_categories: 5 # No longer used for HDF5 direct loading
token_chunk_size: 100 # Expected length of token sequences from HDF5
# cache_dir: "./data/cache" # No longer used for HDF5 direct loading

# Path to the raw dataset cache or original dataset files
path: "data/raw/arxiv_dataset"

# Name of the Hugging Face dataset to load (e.g., "ccdv/arxiv-classification")
name: "ccdv/arxiv-classification"

# Subset of the dataset, if applicable (e.g., "no_ref" for ccdv/arxiv-classification)
subset: "no_ref"

# Cache directory for Hugging Face datasets
cache_dir: "data/cache"

# Path to the Hugging Face tokenizer to be used for the initial Pythia models if not using custom.
# For custom tokenizer, this might be less relevant if custom_tokenizer_path is primary.
# tokenizer_name: "EleutherAI/pythia-70m"

# Base path for the tokenized and chunked HDF5 output files.
# The script will append "_<split_name>.hdf5" (e.g., "_train.hdf5", "_validation.hdf5") 
# or just ".hdf5" if target_split is 'all'.
custom_hdf5_chunked_output_path_base: "data/custom_tokenized_data_chunked_len100"

# Target data split to process by the batch_tokenize_dataset.py script.
# Options: "train", "validation", "test", "all". 
# If "all", all splits are processed into a single HDF5 file (the default behavior if this key is omitted from overrides).
# If a specific split is named, only that split is processed into an HDF5 file named e.g. "..._validation.hdf5".
target_split: "all" # Default to processing all splits into one file

# Categories to filter by (applied during dataset loading by ArxivDatasetLoader if categories_to_keep is null there)
# This is a general list; specific experiments might override active_categories.
# For batch_tokenize_dataset.py, it currently uses a hardcoded list for filtering before tokenization.
# This config entry is more for documentation or if ArxivDatasetLoader defaults to it.
# categories_to_filter: ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.RO"]

# Number of documents to process (for quick testing, 0 means all)
# This is typically used by ArxivDatasetLoader directly.
# num_documents_to_process: 0 

# For HDF5WindowLoader - path to the pre-chunked HDF5 file to be used for training/evaluation.
# This will often be the output of batch_tokenize_dataset.py.
# This should point to the *specific* HDF5 file to use (e.g., the one for 'train' or 'all').
hdf5_chunked_output_path: "${dataset.custom_hdf5_chunked_output_path_base}.hdf5"

# Path for the validation HDF5 file (can be generated by running batch_tokenize_dataset.py with target_split: validation)
validation_hdf5_path: "${dataset.custom_hdf5_chunked_output_path_base}_validation.hdf5"

# hdf5_chunked_output_path: "data/tokenized_data_chunked_len100.hdf5" # Old key, consolidated to 'path'
# custom_hdf5_chunked_output_path: "data/custom_tokenized_data_chunked_len100.hdf5" # Old key, consolidated to 'path'

# Add other dataset related parameters here, e.g., train/val/test splits if using different files per split
# For now, assuming 'path' points to the training data. Validation/test would need separate handling.

# Specify text column for dataset loading and analysis
# text_column: "text" # Not directly applicable for token_ids from HDF5
# label_column: "label" # Not directly applicable for token_ids from HDF5

# Top N categories to select (can be overridden)
# top_n_categories: 5 # This is now hardcoded in batch_tokenize_dataset.py for the main run 

# num_proc for .map() if we were using Hugging Face datasets, not relevant for HDF5SequentialDataset
# num_proc: 4