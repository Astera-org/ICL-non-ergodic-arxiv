Project Title: Focused In-Context Loss Analysis for Single Orchestrator Run

Objective:
Perform an in-context learning analysis by calculating cross-entropy loss at each token position on test data. This analysis will focus exclusively on models from the individual training runs contained within the S3 orchestrator run: `run_2025-05-11_05-33-44_final_python_orch`. This is research code, aiming for clarity and ease of analysis over production-level optimization.

Key Requirements:

1.  Targeted Data Handling:
    *   The project scope is limited to the S3 orchestrator run: `s3://obelisk-simplex/non-ergodic-arxiv/training_runs/run_2025-05-11_05-33-44_final_python_orch/`.
    *   A core `analysis_script.py` will be developed to handle data and model interactions. This script will be structured with VS Code cells (# %%) to allow for step-by-step execution and immediate verification of outputs.
    *   The script will include a function (e.g., `list_s3_training_runs`) to list all individual training run subfolders (e.g., `k1_s0_lr.../`, `k11_s1_lr.../`) directly within the specified orchestrator folder. Each step's output should be easily verifiable in the terminal.
    *   Another function (e.g., `get_train_run_metadata`) will identify and collate S3 paths for essential files from each training run. These paths will be stored in a structured manner.
    *   Essential files include:
        *   Model files (e.g., `config.json`, `model.safetensors` or `pytorch_model.bin`) from `best_model/` (or preferred location if `best_model/` isn't standard or present).
        *   Experiment arguments file (e.g., `_args.json`).
        *   Checkpoint files (format to be determined by inspection).
    *   A function (e.g., `get_train_run_model_files`) will handle on-demand downloading of these files using the collated S3 paths. Files will be organized locally, maintaining a structure like: `analysis/downloaded_data/run_2025-05-11_05-33-44_final_python_orch/k1_s0_lr.../`.

2.  Model Loading and Preparation:
    *   The `analysis_script.py` will contain a function (e.g., `load_model`) to load a Hugging Face transformer model using the downloaded configuration and model weights for a specified individual run (best model or a specific checkpoint).
    *   A function (e.g., `create_test_dataloader`) will adapt `random_window_dataset.py` (or a similar method) to prepare a test dataset.

3.  In-Context Loss Evaluation:
    *   The `analysis_script.py` will feature a function (e.g., `get_in_context_loss`) that, for each loaded model:
        *   Iterates through sequences from the test dataset.
        *   Calculates and stores the cross-entropy loss for each token, given its preceding context.
        *   Primary output: loss profiles (loss per token position).
    *   The implementation of loss calculation and subsequent data handling can draw inspiration from `tests/test_pretrained_in_context_loss.py`.

4.  Results Output and Visualization:
    *   Store the calculated in-context loss profiles in a structured format (e.g., a JSON file for each evaluated model, or an aggregated collection).
    *   Develop a utility to generate plots visualizing these loss profiles, for example, average loss versus token position. The plotting logic in `tests/test_pretrained_in_context_loss.py` serves as a good reference.

Assumptions:
*   AWS S3 access is configured.
*   The models of interest are standard Hugging Face transformer architectures. 

Project Structure and Development Approach:
*   All project-related code, scripts, and outputs should be organized within a root folder named `analysis/`.
*   The primary analysis tool will be `analysis/analysis_script.py`. This script will be composed of clear, modular functions, with each major step encapsulated in a VS Code cell (# %%) for iterative development, execution, and immediate output verification.
*   Other supporting Python code should be structured into modular and clear files (i.e., "nice function files") within the `analysis/` directory as needed.
*   The implementation should prioritize simplicity, straightforwardness, and verifiability at each step.
*   The overall nature of the project is for research purposes, not production deployment. 